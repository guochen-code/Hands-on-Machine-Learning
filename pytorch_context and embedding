******* embedding = context[dataset] ******* embedding = vocabulary[dataset]

import torch

# context
C = torch.randn([27,2])

# dataset
X = torch.tensor([[1,2,3],[4,5,6]])

# embedding
C[X]
tensor([[[ 0.6287, -0.4043],
         [ 0.0220,  0.3561],
         [ 0.8565,  0.0421]],

        [[-0.0779,  0.4526],
         [-0.1771, -0.8766],
         [ 0.1971, -0.8207]]])

# C
tensor([[-3.3001e-01,  2.3160e+00],
        [ 6.2867e-01, -4.0425e-01],
        [ 2.2006e-02,  3.5610e-01],
        [ 8.5651e-01,  4.2141e-02],
        [-7.7878e-02,  4.5262e-01],
        [-1.7711e-01, -8.7658e-01],
        [ 1.9712e-01, -8.2070e-01],
        [-9.1143e-04,  1.6432e+00],
        [-2.4249e-01,  3.2047e-01],
        [-4.7615e-01,  9.3635e-01],
        [ 6.8017e-01,  9.6754e-02],
        [ 5.9520e-01,  7.2067e-01],
        [ 5.4465e-01, -1.5832e+00],
        [ 4.2676e-01, -1.1476e+00],
        [-4.7571e-01,  7.0542e-02],
        [ 6.5496e-01, -4.6445e-01],
        [ 7.4458e-01, -4.3497e-01],
        [ 5.4511e-01,  7.9433e-01],
        [ 9.3047e-01, -6.7856e-01],
        [ 1.0747e+00,  9.0291e-01],
        [ 6.8004e-01, -4.2045e-01],
        [ 4.5865e-01,  2.7620e-01],
        [-1.2305e+00, -1.4768e+00],
        [-1.9166e+00, -3.0216e-02],
        [-1.3407e+00, -1.1753e+00],
        [ 2.3867e+00,  9.6543e-01],
        [-1.5525e+00, -1.1028e+00]])
