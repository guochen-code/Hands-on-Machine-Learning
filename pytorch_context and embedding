******* embedding = context[dataset] ******* embedding = vocabulary[dataset]

import torch

# context
C = torch.randn([27,2])

# dataset --- !!! 2 samples !!!
X = torch.tensor([[1,2,3],[4,5,6]])

# embedding : torch.Size([2, 3, 2])
C[X]
tensor([[[ 0.6287, -0.4043],
         [ 0.0220,  0.3561],
         [ 0.8565,  0.0421]],

        [[-0.0779,  0.4526],
         [-0.1771, -0.8766],
         [ 0.1971, -0.8207]]])

# C
tensor([[-3.3001e-01,  2.3160e+00],
        [ 6.2867e-01, -4.0425e-01],
        [ 2.2006e-02,  3.5610e-01],
        [ 8.5651e-01,  4.2141e-02],
        [-7.7878e-02,  4.5262e-01],
        [-1.7711e-01, -8.7658e-01],
        [ 1.9712e-01, -8.2070e-01],
        [-9.1143e-04,  1.6432e+00],
        [-2.4249e-01,  3.2047e-01],
        [-4.7615e-01,  9.3635e-01],
        [ 6.8017e-01,  9.6754e-02],
        [ 5.9520e-01,  7.2067e-01],
        [ 5.4465e-01, -1.5832e+00],
        [ 4.2676e-01, -1.1476e+00],
        [-4.7571e-01,  7.0542e-02],
        [ 6.5496e-01, -4.6445e-01],
        [ 7.4458e-01, -4.3497e-01],
        [ 5.4511e-01,  7.9433e-01],
        [ 9.3047e-01, -6.7856e-01],
        [ 1.0747e+00,  9.0291e-01],
        [ 6.8004e-01, -4.2045e-01],
        [ 4.5865e-01,  2.7620e-01],
        [-1.2305e+00, -1.4768e+00],
        [-1.9166e+00, -3.0216e-02],
        [-1.3407e+00, -1.1753e+00],
        [ 2.3867e+00,  9.6543e-01],
        [-1.5525e+00, -1.1028e+00]])

******************************************

C[X].shape ---> torch.Size([2, 3, 2])
emb = C[X]
so input to next neural layer = 3*2 = 6
W = torch.rand((6,100))
b = torch.rand(100) 
emb @ W + b # this is what we want, but not work, need to contatenate (2,3,2) -> (2,6)
emb.view(2,6) @ W + b # this will work; view is more efficient than concat and unbind functions in torch

h = torch.tanh(emb.view(-1,6)@W+b) # -1: infer; output shape(2,100)

# softmax layer
W2 = torch.randn((100,27))
b2 = torch.randn(27) # 27 classes
logits = h@ W2 + b2
logits.shape ---> (2,27)
counts = logits.exp()
prob = counts / counts.sum(1, keepdims=True)
prob.shape ---> (2,27)
Y.shape ---> (1,2) # Y is actual label
******************************************************************************************************************************
prob[torch.arange(2),Y] # probability for the correct token for sample 1 and sample 2
******************************************************************************************************************************

loss = - prob[torch.arange(2),Y].log().mean() # negative log likelihood

# summary:
use pytorch's cross_entrophy(), instead of using your own code above. why?
(1) efficiency: less memory, less intermediate variables, less tensors created / efficient back propogation using simplified mathmatical expression (think using gradient descent vs one-step matrix calculation to calculate the min)
(2) significantly "numerically well behaved":
# notice: +/- any number to logits will have the same probs
to avoid extremely small numbers or big numbers, in pytorch, it first looks for the max value in the logits tensor and subtracted max value from logits


