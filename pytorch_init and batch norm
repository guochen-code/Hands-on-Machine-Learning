1. initialization -> a good initialization lead to more productive training time -> better model
the importance of good initialization is down graded because of other techniques like batch normalization and better optimizer

2. batch normalization - control statistics of of the activations in the neural network - two parameters - two buffers
(1) side effect: mean and standard deviation is calculated in each batch, it is random
to one specific sample in different batches, it will behave differently
this is somehow acting like regularization, prevent overfitting, and good in practice

(2) scale and shift distribution -gain and bias - "two learning parameters"
bngain = torch.ones((1,n_hidden))
bnbias = torch.zeros((1,n_hidden))

parameters = [C, W1, b1, W2, b2, bngain, bnbias]

bnmean = hpreact.mean(0,keepdim=True) # along axis=0
bnstd = hpreact.std(0,keepdim=True)
hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias

# how do you do for inference?

(3) two buffers - integrate batch norm in training step rather than calculated after training -- this is for inference stage
bnmean_running = torch.zeros((1,n_hidden))
bnstd_running = torch.ones((1,n_hidden))

bnmeani = hpreact.mean(0,keepdim=True) 
bnstdi = hpreact.std(0,keepdim=True)
hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias
with torch.no_grad():
  bnmean_running = 0.999*bnmean_running + 0.001*bnmeani
  bnstd_running = 0.999*bnstd_running + 0.001*bnstdi

(4) the bias in any weighted layer before batch norm can be removed, it will have no effect at all. The desired effect of bias is inlcuded in batch norm.
emb=C[Xb]
embcat = emb.view(emb.shape[0],-1)
hpreact = embcat @ W1 + b1
bnmean = hpreact.mean(0,keepdim=True) # along axis=0
bnstd = hpreact.std(0,keepdim=True)
hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias # all the + b1 will be offset by - bnmean here # we have own bnbias here
