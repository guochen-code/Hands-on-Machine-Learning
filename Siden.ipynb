{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39fea56-89f2-473f-9f98-e5d76ba8e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030014d-352d-48c5-a024-a9c25b2e74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '16000_pcm_speeches'\n",
    "audito_folder = 'audio'\n",
    "noise_folder = 'noise'\n",
    "\n",
    "audio_path = os.path.join(data_directory, audio_folder)\n",
    "noise_path = os.path.join(data_directory, nosie_fodler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5947e-7fb6-4f21-a2ab-b84495d61bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750eb91f-b6ca-4e1e-a8a8-34171372b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522f7b0-e4fe-4fc4-bd58-65e3dce7c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicefile_names=os.listdir('1600_pcm_speeches/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb82661-a8d7-43a8-9b0a-8d11a74e7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicefile_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad8fcb-9beb-44c6-bd36-9373cb2a1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisefile_names = os.listdir('1600_pcm_speeches/noise')\n",
    "noisefile_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7316a-07a8-4eac-b384-9577773eb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_files_count=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf62139-5868-454e-a957-7ab3ac664eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in voicefile_names:\n",
    "    p=os.listdir('1600_pcm_speeches/audio'+'/'+t)\n",
    "    len_class=len(p)\n",
    "    voice_files_count.append(len_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d11c12-05ea-419c-9d4e-e2b313827433",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ec021-e78f-4e6a-91ce-2a824589dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_files_count=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c9988-50c8-4b76-99cd-9f6ddbc24359",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in voicefile_names:\n",
    "    p=os.listdir('1600_pcm_speeches/noise'+'/'+t)\n",
    "    len_class=len(p)\n",
    "    voice_files_count.append(len_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c7fda-3daa-4eb0-9e7b-80df10dac7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b510b9-67af-4593-a90f-35bd0006ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy\n",
    "\n",
    "random_x=voicefile_names\n",
    "random_y=voice_files_count\n",
    "\n",
    "fig=px.bar(random_x,y=random_y)\n",
    "fig.show() # bars show balanced voice samples for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d4372-1a43-4e11-ac87-9877a5e73719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy\n",
    "\n",
    "random_x=noisefile_name\n",
    "random_y=noise_files_count\n",
    "\n",
    "fig=px.bar(random_x,y=random_y)\n",
    "fig.show() # bars show balanced voice samples for each person\n",
    "# only two noise files but each of them are very lengthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f269-72cd-4a52-83c1-7565f701731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split=0.1 # train validation split\n",
    "shuffle_seed=43 # shuffle data\n",
    "sample_rate=16000\n",
    "scale=0.5 # amplitude\n",
    "batch_size=128 # depends on computer\n",
    "epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f4112-d717-4349-97e7-1b407e9a076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all noise files\n",
    "noise_paths=[]\n",
    "for subdir in os.listdir(noise_path):\n",
    "    subdir_path=Path(noise_path) / subdir\n",
    "    if os.path.isdir(subdir_path):\n",
    "        noise_paths+= [\n",
    "            os.path.join(subdir_path, filepath)\n",
    "            for filepath in os.listdir(subdir_path)\n",
    "            if filepath.endwith('.wav')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d40b20-388f-4b72-a81e-acf1f3921408",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba78a2-0f6b-4ac1-86ba-46b34dc54e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "x,sr=librosa.load('1600_pcm_speeches/noise/other/exercise_bike.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1915e-7eb6-4307-803f-3220e6845839",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(sr) # normally 22050, we need to convert to 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b3378-f3f3-4f55-97be-ba5e34a015f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933ecce-750a-47c3-bd46-caa77c55f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(size=14,5))\n",
    "librosa.display.waveshow(x,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4eb1cf-a98c-4e10-962b-9d59ee38a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=librosa.stft(x)\n",
    "Xdb=librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14,5))\n",
    "librosa.display.sepcshow(Xdb, sr=sr, x_axis='time', y_axis='hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e0abd-15b2-46c9-819b-e88203365ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio('1600_pcm_speeches/noise/other/exercise_bike.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c056d9d-5387-4738-9cad-90c2f93e8db5",
   "metadata": {},
   "source": [
    "### split noise into chunks of 16,000 steps each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cb6a5-203b-4d68-9dda-ba577da05e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(command)\n",
    "# cut noise samples into smaller samples\n",
    "def load_noise_sample(path):\n",
    "    sample,sampling_rate=tf.audio.decode_wav(tf.io.read_file(path),desired_channels=1)\n",
    "    print('sampling rate of original audio',sampling_rate)\n",
    "    if sampling_rate = sample_rate:\n",
    "        print('shape',sample.shape[0])\n",
    "        slices=int(sample.shape[0]/sample_rate)\n",
    "        print(slices)\n",
    "        sample=tf.split(sample[:slices*sample_rate],slices)\n",
    "    else:\n",
    "        print('sampling rate for', path, 'is incorrect')\n",
    "        return None\n",
    "    \n",
    "noises=[]\n",
    "for path in noise_paths:\n",
    "    sample=load_noise_sample(path)\n",
    "    if sample:\n",
    "        noises.extend(sample)\n",
    "noises=tf.stack(noises)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11697a-d979-42ab-adc2-87cd1eaf3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77624a0-5187-4d86-b666-1eb70205053f",
   "metadata": {},
   "source": [
    "### dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc31d1-da3d-4504-9f91-acbeb69c71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths,labels):\n",
    "    path_ds=tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds=path_ds.map(lambdax:path_to_audio(x))\n",
    "    label_ds=tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip(audio_ds,label_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721fed2-a4c4-473c-bc7a-0b89cb283f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    audio=tf.io.read_file(path)\n",
    "    audio,_=tf.audio.decode_wav(audio,1,sample_rate)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678df4d-2402-4e34-aff5-4b879ac21cd8",
   "metadata": {},
   "source": [
    "### add noise to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80b718-76d1-403d-ba28-3ba2efee0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(audio, noises=None, scale=0.5):\n",
    "    if noises is not None:\n",
    "        tf_rnd=tf.random.uniform((tf.shape[0],),0.noises.shape[0],dtype=tf.int32)\n",
    "        noise=tf.gather(noises,tf_rnd,axis=0)\n",
    "        prop=tf.math.reduce_max(audio,axis=1)/tf.math.reduce_max(noise,axis=1)\n",
    "        prop=ft.repeat(tf.expand_dims(prop,axis=1),tf/shape(audio)[1],axis=1)\n",
    "        audio=audio+noise*prop*scale\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700ba2b-0b54-4fa1-b6f7-05cc3b8ed997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_fft(audio):\n",
    "    audio=tf.squeeze(audio,axis=-1)\n",
    "    fft=tf.signal.fft(tf.cast(tf.complex(real=audio,imag=tf.zeros_like(audio)),tf.complex64))\n",
    "    fft=tf.expand_dims(fft,axis=-1)\n",
    "    return tf.math.abs(fft[:,:(audio.shape[1]//2),:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059fb1a-144f-446b-98cf-3f886e86cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=os.listdir(audio_path)\n",
    "print(class_names,)\n",
    "\n",
    "audio_paths=[]\n",
    "labels=[]\n",
    "for label,name in enumerate(class_names):\n",
    "    print('speaker:', (name))\n",
    "    dir_path=Path(audio_path)/name\n",
    "    speaker_sample_paths=[os.path.join(dir_path,filepath) for filepath in os.listdir(dir_path) if filepath.endswith('.wav')]\n",
    "    audio_paths+=speaker_sample_paths\n",
    "    labels+=[label]*len(spearker_sample_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775312f8-3507-4997-9ac5-460b058e917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle to generate random data\n",
    "rng=np.random.RandomState(shuffle_seed)\n",
    "rng.shuffle(audio_paths)\n",
    "rng=np.random.RandomState(shuffle_seed)\n",
    "rng.shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d248df8-6bbf-4a58-84b1-009135038b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation\n",
    "num_val_samples=int(valid_split*len(audio_paths))\n",
    "train_audio_paths=audio_paths[:-num_val_samples]\n",
    "train_labels=labels[:-num_val_samples]\n",
    "\n",
    "valid_audio_paths=audio_paths[-num_val_sample:]\n",
    "valid_labels=labels[-num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff3ac5-61bf-4808-b33b-9bb2894e5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets, one for trainig and the other for validation\n",
    "train_ds=paths_and_labels_to_dataset(train_audio_paths,train_labels)\n",
    "train_ds=train_ds.shuffle(buffer_size=batch_size*8, seed=shuffle_seed).batch(batch_size)\n",
    "\n",
    "valid_ds=paths_and_labes_to_dataset(valid_audio_paths,valid_labels)\n",
    "valid_ds=valid_ds.shuffle(buffer_size=32*8, seed=shuffle_seed).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c16a2-c3b5-4f06-82b4-69285bd7303d",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40294c-b9fd-41fd-aa63-b8ed85814265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise tot the training set\n",
    "train_ds=train_ds.map(lambda x,y:(add_noise(x,noises,scale=scale),y),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE,)\n",
    "\n",
    "#transform audio wave to the frequency domain using 'audio_to_fft'\n",
    "train_ds=train_ds.map(lambda x,y: (audio_to_fft(x),y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds=train_ds.prefetch(ft.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_ds=valid_ds.map(lambda x,y: (audio_to_fft(x),y),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_ds=valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8b35e-43d2-45b8-9b08-f001f8570497",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65de87-0418-4ace-b605-7df892e94790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cafb9e-8173-43f1-b61e-cf84e3d748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, conv_num=3,activation='relu'):\n",
    "        s=keras.layers.Conv1D(filters,1,padding='same')(x)\n",
    "        \n",
    "        for i in range(conv_num-1):\n",
    "            x=keras.layers.Conv1D(filters,3,padding='same')(x)\n",
    "            x=keras.layers.Activation(activation)(X)\n",
    "            \n",
    "        x = keras.layers.Conv1D(filters,3,padding='same')(x)\n",
    "        x=keras.layers.Add()([x,s])\n",
    "        x=keras.layers.Activation(activation)(x)\n",
    "        \n",
    "        return keras.layers.MaxPool1D(pool_size=2,strides=2)(x)\n",
    "\n",
    "def build_model(input_shape,num_classes):\n",
    "    inputs=keras.layers.Input(shape=input_shape,name='input')\n",
    "    \n",
    "    x=residual_block(inputs,16,2)\n",
    "    x=residual_block(inputs,32,2)\n",
    "    x=residual_block(inputs,64,3)\n",
    "    x=residual_block(inputs,128,3)\n",
    "    x=residual_block(inputs,128,3)\n",
    "    x=keras.layers.AveragePooling1D(pool_size=3,strides=3)(x)\n",
    "    x=keras.layers.Flatten()(x)\n",
    "    x=keras.layers.Dense(256,activation='relu')(x)\n",
    "    x=keras.layers.Dense(128,activation='relu')(x)\n",
    "    \n",
    "    outputs=keras.layers.Dense(num_class,activation='softmax', name='output')(x)\n",
    "    return keras.model.Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "model.build_model((sample_rate//2,1),len(class_names))\n",
    "model.summary()\n",
    "model.compile(optiizer='Adam',loss='sparse_categorical_crossentropy', metrics-['accuracy'])\n",
    "model_save_filename='model.h5'\n",
    "earlystipping_cb=keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb=keras.callbacks.ModelCheckpoint(model_save_filename,monitor='val_accuracy',save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360ba8d-11d0-4023-b4d1-f3f329124078",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d9137-b62f-496f-aba7-7977971dffd2",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbc697-f1f6-431b-b989-b812a51267c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(\n",
    "    trian_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cfbc2-52b2-45d6-b796-6650a961e84a",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea3a5a-69da-48db-a470-0587b1dcf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of model:', model.evaluate(valid_ds)) # loss: 0.0914; accuracy: 0.9800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c75be-080c-4182-8582-c948211970a6",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3b65a-1903-4106-8ae1-91a29b680094",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_TO_DISPLAY=10\n",
    "\n",
    "test_ds=paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "test_ds=test_ds.shuffle(buffer_size=batch_size*8, seed=shuffle_seed).batch(batch_size)\n",
    "\n",
    "test_ds=test_ds.map(lambda x,y: (add_noise(x, noises, scale=scale),y))\n",
    "\n",
    "for audios,labels in test_ds.take(1):\n",
    "    ffts=audio_to_fit(audios)\n",
    "    y_pred=model.predict(ffts)\n",
    "    rnd=np.random.randint(0,batch_size,SAMPLES_TO_DISPLAY)\n",
    "    audios=audios.numpy()[rnd,:,:]\n",
    "    labels=labels.numpy()[rnd]\n",
    "    y_pred=np.argmax(y_pred,axis=-1)[rnd]\n",
    "    \n",
    "    for index in range(SAMPLES_TO_DISPLAY):\n",
    "        print('speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m'.format(\n",
    "            \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "            class_names[labels[index]],\n",
    "            \"[92m\" if labels[index] == y_pre[index] else \"[91m\",\n",
    "            class_names[y_pred[index]],\n",
    "        )\n",
    "             )\n",
    "        if labels[index] == y_pred[index]:\n",
    "            print('welcome')\n",
    "        else:\n",
    "            print('sorry')\n",
    "        print(\"The speaker is\" if labels[index] == y_pred[iondex] else \"\", class_names[y_pred[index]]) ######################\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bde762-60f4-4bbc-8fd7-12ec4f6c520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "spkear: Julia_Gillard Predicted: Julia_gillard\n",
    "welcome\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be67ba9-e877-4d79-9bde-7891e49a53c7",
   "metadata": {},
   "source": [
    "### predict the speaker from test dataset for real time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b0dc0-969f-4eae-9fa2-62aea21e446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataset(audio_paths):\n",
    "    path_ds=tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    return tf.data.Dataset.zip((path_ds))\n",
    "\n",
    "def predict(path,labels):\n",
    "    test = paths_and_labels_to_dataset(path,labels)\n",
    "    \n",
    "    test=test.shuffle(buffer_size=batch_size*8,seed=shuffle_seed).batch(batch_size)\n",
    "    test=test.prefetch(ft.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    test=test.map(lambda x,y: (add_noise(x,noises,scale=scale),y))\n",
    "    \n",
    "    for audios, labels in test.take(1):\n",
    "        ffts=audio_to_fft(audios)\n",
    "        y_pred=model.predict(ffts)\n",
    "        rnd=np.random.randint(0,1,1)\n",
    "        audios=audios.numpy[rnd,;]\n",
    "        labels=labels.numpy[rnd]\n",
    "        y_pred=np.argmax(y_pred, axis=-1)[rnd]\n",
    "        \n",
    "    for index in range(1):\n",
    "        print(\n",
    "            \"speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
    "                \"[92m\",y_pred[index],\n",
    "                \"[92m\",y_predi[index])\n",
    "        )\n",
    "        print(\"speaker predicted:\", class_names[y_pred[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f1377-9ca1-4b4b-86ec-614563fe3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=['1600_pcm_speeches/audio/Jens_Stoltenberg/1013.wav']\n",
    "labels=['unknow']\n",
    "try:\n",
    "    predict(path,labels)\n",
    "except:\n",
    "    print('Error! check if the file is correctly passed or not!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
