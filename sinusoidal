PE(pos, 2i) = sin(pos / (10000^(2i/d_model)))
PE(pos, 2i+1) = cos(pos / (10000^(2i/d_model)))
where d_model is the dimensionality of the input embedding.
The index 'i' represents the index of the dimension in the positional encoding vector, and it should actually range from 0 to (d_model/2) - 1.

##############################################################################################################################################################################

By using different frequencies (controlled by the index 'i') and scaling factors (determined by the dimension 'd_model'), 
the sinusoidal functions create distinct patterns that help differentiate positions in the sequence. 
This uniqueness is crucial for the model to capture the positional information and generalize it effectively.

The index 'i' represents the index of the dimension in the positional encoding vector, and it should actually range from 0 to (d_model/2) - 1.

##############################################################################################################################################################################

Step 1: Tokenization
The sentence is first tokenized into individual words, resulting in the following tokens: ['I', 'love', 'natural', 'language', 'processing'].

Step 2: Word Embedding (because it is 4D, so positional vector must be 4D)
Each token is embedded into a fixed-size vector representation called a word embedding. Let's assume each word is embedded into a 4-dimensional vector.

Step 3: Positional Encoding
For each position in the input sequence, we compute a unique positional encoding vector.

Assuming a maximum sequence length of 5 (including padding), the positional encoding vectors are calculated as follows:

Position 0:

PE(0, 0) = sin(0 / (10000^(0/4))) = 0
PE(0, 1) = cos(0 / (10000^(0/4))) = 1
PE(0, 2) = sin(0 / (10000^(2/4))) = 0
PE(0, 3) = cos(0 / (10000^(2/4))) = 1
Position 1:

PE(1, 0) = sin(1 / (10000^(0/4))) = 0.84147098
PE(1, 1) = cos(1 / (10000^(0/4))) = 0.54030231
PE(1, 2) = sin(1 / (10000^(2/4))) = 0.90929743
PE(1, 3) = cos(1 / (10000^(2/4))) = -0.41614684
Position 2:

PE(2, 0) = sin(2 / (10000^(0/4))) = 0.90929743
PE(2, 1) = cos(2 / (10000^(0/4))) = -0.41614684
PE(2, 2) = sin(2 / (10000^(2/4))) = -0.7568025
PE(2, 3) = cos(2 / (10000^(2/4))) = -0.65364362
Position 3:

PE(3, 0) = sin(3 / (10000^(0/4))) = -0.7568025
PE(3, 1) = cos(3 / (10000^(0/4))) = -0.65364362
PE(3, 2) = sin(3 / (10000^(2/4))) = 0.98935825
PE(3, 3) = cos(3 / (10000^(2/4))) = -0.14550003
Position 4:

PE(4, 0) = sin(4 / (10000^(0/4))) = -0.95892427
PE(4, 1) = cos(4 / (10000^(0/4))) = 0.28366219
PE(4, 2) = sin(4 / (10000^(2/4))) = 0.41211849
PE(4, 3) = cos(4 / (10000^(2/4))) = 0.91151115

##############################################################################################################################################################################
summary:
position 0 - 4 : fixed max sequence length is 5
PE(0,0) - (0,3) : 4D vector, same as embeded vector


in multi-head attention, each head has its own set of learned weight matrices for key, value, and query. 
This means that for each head, there are separate sets of parameters that are used to transform the input embeddings into key, value, and query vectors.
Overall, the use of multiple heads with separate key, value, and query matrices allows the model to capture different patterns and dependencies in the data, 
enhancing its ability to learn and represent complex relationships within the input sequence.


Yes, that's correct. The key, value, and query vectors are derived from the input embedding vector, 
but in the attention calculation, only the key, value, and query vectors are used to compute the attention scores. 
The input embedding vector itself is not directly involved in the calculation of attention scores.

The purpose of deriving separate key, value, and query vectors from the input embedding vector is to provide different representations or views of 
the input that capture different aspects or dimensions of information. The key and value vectors help determine the relevance or importance of each value 
in relation to a given query, while the query vector represents the context or information being queried.

By separating the key, value, and query vectors and allowing them to interact through the attention mechanism, 
the model can capture complex relationships and dependencies between different parts of the input sequence, 
facilitating effective information processing and context understanding.

#### word embedding:
So, to summarize:
The values of the embeddings themselves are initially random and have no inherent semantic meaning.
The weights between the input layers and the next layers are learned and adjusted during training to capture the relationships and patterns in the data.
The adjustments of these weights are done through backpropagation and gradient descent to optimize the model's performance.
